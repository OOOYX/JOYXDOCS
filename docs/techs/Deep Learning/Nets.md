## 池化和卷积的异同

- 池化保留重要特征（聚合特征），降维，过滤噪声
- 卷积提取特征

## AlexNet

- **使用ReLU激活函数**：相比传统的 Sigmoid 或 Tanh 激活函数，ReLU 在训练深层网络时能更有效地防止梯度消失的问题。
- **使用Dropout技术**：以一定的概率临时丢弃网络中的部分神经元，防止模型过拟合。
- **局部响应归一化 (LRN)**：尽管后来的研究表明 LRN 并非必需，但在当时，这一技术被认为能提升模型的泛化能力。
- **重叠的池化层**：AlexNet 使用的池化层步长小于池化核尺寸，增加了特征的重叠和覆盖范围，有助于提取更多的特征信息。

## VGGNet

- **统一的卷积层设计**：VGGNet 采用了多个相同尺寸的卷积核（主要是 3x3），层与层之间只通过增加卷积核的数量来扩展网络深度。
- **更深的网络结构**：通过重复使用小卷积核和增加网络深度，VGGNet 能够在保持较小感受野的同时，捕捉更复杂的图像特征。
- **使用MaxPooling**：在卷积层组之间使用最大池化层进行下采样，减少特征维度，减轻计算负担。

## ResNet

网络结构上，ResNet是在普通网络层之间增加了短路连接。

![img](https://i-blog.csdnimg.cn/blog_migrate/9799d2b846583581f8ecb964e968f657.png)

ResNet能够较好地解决层数增加时带来的梯度消失或梯度爆炸的问题，从而能构建有效的更多层数的网络。
