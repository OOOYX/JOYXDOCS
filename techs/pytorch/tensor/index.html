<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta content="text/html; charset=utf-8" http-equiv="Content-Type">
<title>Tensor - JOYXDOCS</title>
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<meta name="generator" content="mkdocs-1.5.3, mkdocs-gitbook-1.0.7">

<link rel="shortcut icon" href="../../../images/favicon.ico" type="image/x-icon">
<meta name="HandheldFriendly" content="true"/>
<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="black">
<meta rel="next" href="" />
<link href="../../../css/style.min.css" rel="stylesheet"> 
</head>

<body>
<div class="book">
<div class="book-summary">
<div id="book-search-input" role="search">
<input type="text" placeholder="Type to search" />
</div> <!-- end of book-search-input -->

<nav role="navigation">
<ul class="summary">
<li>
<a href="../../.." target="_blank" class="custom-link">JOYXDOCS</a>
</li>
<li class="divider"></li>
<li class="chapter" data-path="">
<a href="../../..">封面</a>
<li class="header">技术积累</li>

<li>
<a href="#">网页开发</a>
<ul>

<li>
<a href="../../web_development/css/" class="">CSS</a>
</li>
</ul>
</li>

<li>
<a href="#">PyTorch</a>
<ul>

<li>
<a href="./" class="active">Tensor</a>
</li>
</ul>
</li>

<li>
<a href="#">DeepLearning</a>
<ul>

<li>
<a href="../../Deep%20Learning/Nets/" class="">网络结构</a>
</li>

<li>
<a href="../../Deep%20Learning/%E5%9F%BA%E4%BA%8E%E6%BD%9C%E5%9C%A8%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%9B%BE%E5%83%8F%E5%90%88%E6%88%90/" class="">基于潜在扩散模型的图像合成</a>
</li>

<li>
<a href="../../Deep%20Learning/Transformer/" class="">Transformer</a>
</li>
</ul>
</li>

<li class="header">其他</li>

<li>
<a href="#">Academic Writing</a>
<ul>

<li>
<a href="../../../other/academic_writing/L1/" class="">Lesson1</a>
</li>

<li>
<a href="../../../other/academic_writing/L2/" class="">Lesson2</a>
</li>

<li>
<a href="../../../other/academic_writing/L3/" class="">Lesson3</a>
</li>

<li>
<a href="../../../other/academic_writing/L4/" class="">Lesson4</a>
</li>
</ul>
</li>

<li class="divider"></li>



<li><a href="http://www.mkdocs.org">
Published with MkDocs
</a></li>

<li><a href="https://github.com/GitbookIO/theme-default">
Theme by GitBook
</a></li>
</ul>

</nav>

</div> <!-- end of book-summary -->

<div class="book-body">
<div class="body-inner">
<div class="book-header" role="navigation">

<!-- Title -->
<h1>
<i class="fa fa-circle-o-notch fa-spin"></i>
<a href="." ></a>
</h1>

</div> <!-- end of book-header -->

<div class="page-wrapper" tabindex="-1" role="main">
<div class="page-inner">
<div id="book-search-results">
<div class="search-noresults">

<section class="normal markdown-section">



<h2 id="tensor">Tensor</h2>
<h3 id="_1">创建张量</h3>
<ul>
<li>torch.ones </li>
<li>torch.empty</li>
<li>torch.zeros</li>
<li>torch.tensor 已有数据创建</li>
</ul>
<pre><code class="language-python"># 创建一个tensor，并设置requires_grad=True以跟踪计算历史  
x = torch.ones(2, 2, requires_grad=True)  
</code></pre>
<h3 id="_2">梯度</h3>
<p>张量的梯度是一个与张量形状相同的张量，表示损失函数（或标量目标函数）对该张量的每个元素的偏导数。梯度是反向传播算法的核心部分，用于更新模型参数以优化（通常是最小化）损失函数。</p>
<pre><code class="language-python">import torch

# 创建一个张量，并设置 requires_grad=True 以跟踪计算历史
x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)

# 定义一个简单的函数 y = 3 * x^2
y = 3 * x ** 2

# 将 y 归约为标量，例如通过求和
z = y.sum()

# 计算 z 对 x 的梯度
z.backward()

# 输出 x 的梯度
print(x.grad)  # 输出: tensor([ 6.0000, 12.0000, 18.0000])
</code></pre>
<p>这一段代码中<code>z.backward()</code>方法的作用是计算标量 <code>z</code> 对张量 <code>x</code> 的梯度。具体来说，它会通过自动微分机制，在计算图上执行反向传播，计算损失函数（在这里是标量 <code>z</code>）对每个叶子张量（在这里是 <code>x</code>）的偏导数，并将这些偏导数存储在相应张量的 <code>.grad</code> 属性中。</p>
<h3 id="backward">.backward()方法</h3>
<pre><code class="language-python">out.backward(torch.tensor([0.1,1.0,1.0]),dtype=float)
</code></pre>
<p>如果<code>out</code>不是一个标量，那么在调用<code>.backward()</code>时需要传入一个与<code>out</code>同形的权重向量进行相乘。</p>
<h2 id="gpu">GPU</h2>
<ul>
<li><code>torch.cuda.is_available()</code>检查GPU是否可用</li>
</ul>
<pre><code class="language-python"># 创建一个tensor  
x = torch.tensor([1.0, 2.0])  

# 移动tensor到GPU上  
if torch.cuda.is_available():  
    x = x.to('cuda')  
</code></pre>
<pre><code class="language-python"># 直接在GPU上创建tensor  
if torch.cuda.is_available():  
    x = torch.tensor([1.0, 2.0], device='cuda')  
</code></pre>
<pre><code class="language-python"># 创建一个简单的模型  
model = torch.nn.Linear(10, 1)  

# 创建一些数据  
data = torch.randn(100, 10)  

# 移动模型和数据到GPU  
if torch.cuda.is_available():  
    model = model.to('cuda')  
    data = data.to('cuda')  
</code></pre>
<h2 id="torchnn">神经网络—torch.nn库</h2>
<p>troch.nn库是用于构建神经网络的工具库</p>
<h3 id="_3">主要组件</h3>
<h4 id="1-layers">1. <strong>神经网络层（Layers）</strong></h4>
<p><code>torch.nn</code> 提供了多种常用的神经网络层，例如：</p>
<ul>
<li><strong>线性层（全连接层）</strong>：</li>
</ul>
<p><code>torch.nn.Linear(in_features, out_features, bias=True)</code></p>
<ul>
<li><strong>卷积层</strong>：</li>
</ul>
<p><code>torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, bias=True)</code></p>
<ul>
<li><strong>池化层</strong>：</li>
</ul>
<p><code>torch.nn.MaxPool2d(kernel_size, stride=None, padding=0)</code></p>
<ul>
<li><strong>批归一化层</strong>：</li>
</ul>
<p><code>torch.nn.BatchNorm2d(num_features)</code></p>
<ul>
<li><strong>激活函数</strong>：</li>
</ul>
<p><code>torch.nn.ReLU(inplace=False)</code></p>
<h4 id="2-loss-functions">2. <strong>损失函数（Loss Functions）</strong></h4>
<p><code>torch.nn</code> 提供了多种损失函数，用于衡量模型预测值和真实值之间的差异，例如：</p>
<ul>
<li><strong>均方误差损失</strong>：</li>
</ul>
<p><code>torch.nn.MSELoss()</code></p>
<ul>
<li><strong>交叉熵损失</strong>：</li>
</ul>
<p><code>torch.nn.CrossEntropyLoss()</code></p>
<ul>
<li><strong>二分类交叉熵损失</strong>：</li>
</ul>
<p><code>torch.nn.BCELoss()</code></p>
<h4 id="3-containers">3. <strong>容器（Containers）</strong></h4>
<p>容器用于将多个层组合在一起，形成一个更大的模型。例如：</p>
<ul>
<li><strong>顺序容器（Sequential）</strong>：</li>
</ul>
<p><code>torch.nn.Sequential(*args)</code></p>
<ul>
<li><strong>模块列表（ModuleList）</strong>：</li>
</ul>
<p><code>torch.nn.ModuleList(modules=None)</code></p>
<ul>
<li><strong>模块字典（ModuleDict）</strong>：</li>
</ul>
<p><code>torch.nn.ModuleDict(modules=None)</code></p>
<h4 id="4-custom-modules">4. <strong>自定义模块（Custom Modules）</strong></h4>
<p>通过继承 <code>torch.nn.Module</code>，可以定义自己的神经网络模块：</p>
<pre><code>import torch.nn as nn

class MyModel(nn.Module):
    def __init__(self):
        super(MyModel, self).__init__()
        self.layer1 = nn.Linear(10, 20)
        self.layer2 = nn.Linear(20, 10)

    def forward(self, x):
        x = self.layer1(x)
        x = nn.ReLU()(x)
        x = self.layer2(x)
        return x
</code></pre>
<h4 id="5-initialization">5. <strong>参数初始化（Initialization）</strong></h4>
<p><code>torch.nn.init</code> 子模块提供了一些常见的参数初始化方法：</p>
<pre><code>import torch.nn.init as init

# 初始化权重为均匀分布
init.uniform_(tensor, a=0.0, b=1.0)

# 初始化权重为正态分布
init.normal_(tensor, mean=0.0, std=1.0)
</code></pre>
<h3 id="_4">典型使用流程</h3>
<p>以下是一个典型的使用 <code>torch.nn</code> 构建和训练神经网络的示例：</p>
<pre><code>import torch
import torch.nn as nn
import torch.optim as optim

# 定义模型
class SimpleModel(nn.Module):
    def __init__(self):
        super(SimpleModel, self).__init__()
        self.fc1 = nn.Linear(10, 50)
        self.fc2 = nn.Linear(50, 1)

    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = self.fc2(x)
        return x

# 实例化模型
model = SimpleModel()

# 定义损失函数和优化器
criterion = nn.MSELoss()
optimizer = optim.SGD(model.parameters(), lr=0.01)

# 生成一些示例数据
inputs = torch.randn(32, 10)
targets = torch.randn(32, 1)

# 训练循环
for epoch in range(100):
    # 前向传播
    outputs = model(inputs)
    loss = criterion(outputs, targets)

    # 反向传播和优化
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    print(f'Epoch [{epoch+1}/100], Loss: {loss.item():.4f}')
</code></pre>
<h3 id="_5">总结</h3>
<p><code>torch.nn</code> 是 PyTorch 中一个功能强大的模块，提供了构建、训练和评估神经网络所需的各种工具。通过结合使用不同的层、损失函数和优化器，你可以构建和训练各种复杂的深度学习模型。理解 <code>torch.nn</code> 的各个组件及其用途是掌握 PyTorch 的关键。</p>


</section>
</div> <!-- end of search-noresults -->
<div class="search-results">
<div class="has-results">

<h1 class="search-results-title"><span class='search-results-count'></span> results matching "<span class='search-query'></span>"</h1>
<ul class="search-results-list"></ul>

</div> <!-- end of has-results -->
<div class="no-results">

<h1 class="search-results-title">No results matching "<span class='search-query'></span>"</h1>

</div> <!-- end of no-results -->
</div> <!-- end of search-results -->
</div> <!-- end of book-search-results -->

</div> <!-- end of page-inner -->
</div> <!-- end of page-wrapper -->

</div> <!-- end of body-inner -->

</div> <!-- end of book-body -->
<script src="../../../js/main.js"></script>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js"></script>
<script src="../../../_js/configure-mathjax.js"></script>
<script src="../../../search/main.js"></script>
<script src="../../../js/gitbook.min.js"></script>
<script src="../../../js/theme.min.js"></script>
</body>
</html>